---
title: "Machine Learning 1"
author: "Tasnia Sharia (PID A15931128)"
date: "10/21/2021"
output:
  pdf_document: default
  html_document: default
---

First is clustering methods

# Kmeans clustering

The function in base R to do Kmenas clustering is called 'kmeans()'

First make up some data where we know what the answer should be:

```{r}
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

> Q. Can we use kmeans() to cluster this data setting k to 2 and nstart to 20?

```{r}
km <- kmeans(x, centers = 2, nstart = 20)
km
```
> Q. How many points are in each cluster?

```{r}
km$size
```

> Q. What 'component' of your result object details cluster assignment/membership?

```{r}
km$cluster
```

> Q. What 'component' of your result object details cluster center?

```{r}
km$centers
```

> Q. Plot x colored by the kmeans cluster assignment and and cluster centers as blue points

```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```

# Hierarchical Clustering

A big limitation with k-means is that we have to tell it K (the number of clusters we want).

Analyze this same data with hclust()

Demonstrate the use of dist(), hclust(), plot(), and cutree() function to do clustering. Generate dendeograms and return cluster assignment membership vector.

```{r}
hc <- hclust( dist(x) )
hc
```

There is a plot method for hclust result objects. Let's see it.

```{r}
plot(hc)
```

To get our cluster membership vector we have to do a wee bit more work. We have to "cut" the tree where we think it makes sense.  For this we use the 'cutree()' function.

```{r}
cutree(hc, h=8)
```

You can also call 'cutree()' settng k= the number of grps/clusters you want

```{r}
grps <- cutree(hc, k=2)
```

Make our results plot

```{r}
plot(x, col=grps)
```


# Now, We will examine the PCA of UK food data

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
View(x)
```

> **Q1.** How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
nrow(x)
ncol(x)
dim(x)
```
The dim() function returns the # of rows and columns.  Using nrow() or ncol() functions also provide the answer

```{r}
## Preview the first 6 rows
head(x)
## Preview the last 6 rows
tail(x)
```

The data should be 17 by 4 dimensions. There is an extra first column that needs to be fixed.
```{r}
# Note how the minus indexing works
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```


Now let's check the dimensions again
```{r}
dim(x)
```

Alternative approach to setting the correct row-names
```{r}
x <- read.csv(url, row.names=1)
head(x)
dim(x)
```

> **Q2.** Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?


The second approach to solving the 'row-names problem' where we include the row.names argument is more convenient. 
```{r}
# What would happened if we ran row names(x) <- x[,1] again
rownames(x) <- x[,1]
head(x)
```

By running rownames(x) <- x[,1] multiple times, the first column from the previous run would be removed.  This would cause a problem of having data being removed after each run when the code is rewritten.

Now we go back to the original data with the right number of dimensions and correct headings.
```{r}
x <- read.csv(url, row.names=1)
head(x)
View(x)
```


We will spot major differences and trends by generating boxplots and various pairwise plots that may not help as much
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

> **Q3:** Changing what optional argument in the above barplot() function results in the following plot?

```{r}
barplot(as.matrix(x), beside=FALSE, col=rainbow(nrow(x)))
```
Changing the beside argument can change the plot. Setting beside argument to FALSE creates a stacked boxplot. Leaving the beside argument out would also give the same result because being FALSE is the default.  If beside=TRUE were to be added, TRUE would be a numeric vector and includes other components that could be usedul information to the data.


> **Q5:** Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
```
It is difficult to make sense of this visual dataset.  I cannot seem to make out what the figures represent.  The axis are very confusing.  The diagonal lines appear to show a trend but can't tell what sort of trend.  


> **Q6.** What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

We cannot tell which figures belong to which countries.  Does each country have its own respective row or column? Which figures belong to which country?


# PCA to the rescue

prcomp() expects the observations to be rows and the variables to be columns therefore we need to first transpose our data.frame matrix with the t() transpose function.

```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
```


> **Q7.** Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```


> **Q8.** Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col=c("orange", "red", "blue", "dark green"))
```

We automatically obtain information about the contributions of each PC to the total variance of the coordinates, which is contained in the Eigenvectors returned from such calculations

We can use the square of pca$sdev , which stands for “standard deviation”, to calculate how much variation in the original data each PC accounts for.

For the prcomp() function we can use the summary() command or examine the returned pca$sdev (see below).

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```
```{r}
## summary() command for prcomp function
z <- summary(pca)
z$importance
```

This information can be summarized in a plot of the variances (eigenvalues) with respect to the principal component number (eigenvector number)

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")

```

# Digging deeper variable loadings
Consider the influence of each of the original variables upon the principal components, known as loading scores. We obtain the info using prcomp() component $rotation. Also summarize with a call to biplot()

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```
Largest positive loading scores “push” N. Ireland to right positive side of the plot because of Fresh_potatoes and Soft_drinks. High negative scores of Fresh_fruit and Alcoholic_drinks push the other countries to the left side of the plot.


> **Q9:** Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

```{r}
## focusing on PC2 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```
Negative loading scores include Soft_drinks, Alcholic_drinks, Processes_potatoes.  High positive scores include Fresh_potatoes, Other_veg, and Cereals


# Another way to see information together with the main PCA plot is in a biplot
```{r}
## The inbuilt biplot() can be useful for small datasets 
biplot(pca)
```
There is a central group of foodsin the middle of each PC, with a few that are far from the group. This could be representative of how England, Wales and Scotland were clustered together being "similar", whilst Northern Ireland was the country that was away from the cluster.



# Now we will look at the PCA of RNA-seq data

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
View(rna.data)
```

> **Q10:** How many genes and samples are in this data set?

```{r}
dim(rna.data)
```
There are 100 genes and 10 samples in this data set.


**Do a PCA and plot results rather than use any other plots**

```{r}
## Again we have to take the transpose of our data 
pca <- prcomp(t(rna.data), scale=TRUE)
 
## Simple un polished plot of pc1 and pc2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")
```

**Let’s examine a summary of how much variation in the original data each PC accounts for**
```{r}
summary(pca)
```
By viewing the cumulative proportion, we see that PC1 captures 92.6% of the action. This means that we have successfully reduced a 100 diminesional data set down to only one dimension that retains 92.6% of the essential features from the original data. PC1 captures 92.6%, PC1 and PC2 together captures 94.9%.


**Quick barplot summary of this Proportion of Variance for each PC**
```{r}
plot(pca, main="Quick scree plot")
```

Square of pca$sdev to calculate how much variation in the original data each PC accounts for
```{r}
## Variance captured per PC 
pca.var <- pca$sdev^2

## Percent variance is often more informative to look at 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```
**Generate another scree plot**
```{r}
barplot(pca.var.per, main="Scree Plot", 
        names.arg = paste0("PC", 1:10),
        xlab="Principal Component", ylab="Percent Variation")
```

**Making the plot appear more useful**
```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca$x[,1], pca$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))
```


# Using ggplot
```{r}
# Prep ggplot
library(ggplot2)
# Contain PCA results in dataframe
df <- as.data.frame(pca$x)
# Our first basic plot
ggplot(df) + 
  aes(PC1, PC2) + 
  geom_point()
```

Adding some aesthetics and color
```{r}
# Add a 'wt' and 'ko' "condition" column
df$samples <- colnames(rna.data) 
df$condition <- substr(colnames(rna.data),1,2)

p <- ggplot(df) + 
        aes(PC1, PC2, label=samples, col=condition) + 
        geom_label(show.legend = FALSE)
p
```

More polishing to the plot
```{r}
p + labs(title="PCA of RNASeq Data",
       subtitle = "PC1 clearly seperates wild-type samples from knock-out samples",
       x=paste0("PC1 (", pca.var.per[1], "%)"),
       y=paste0("PC2 (", pca.var.per[2], "%)"),
       caption="BIMM143 RNASeq example data") +
     theme_bw()
```


Let’s find the top 10 measurements (genes) that contribute most to pc1 in either direction
```{r}
loading_scores <- pca$rotation[,1]

## Find the top 10 measurements (genes) that contribute
## most to PC1 in either direction (+ or -)
gene_scores <- abs(loading_scores) 
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)

## show the names of the top 10 genes
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes 
```

