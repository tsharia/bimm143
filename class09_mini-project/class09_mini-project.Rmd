---
title: 'Class09: Mini Project'
author: "Tasnia Sharia (PID A15931128)"
date: "10/26/2021"
output:
  pdf_document: default
  html_document: default
---

# In this project, we will be analyzing data describing the characteristics of cell nuclei found in breast cancer

We first need to download and import the data that is already located in the project directory
```{r}
#save the data as a variable
data <- "WisconsinCancer.csv"
#inputting data and ensuring column names are set correctly
wisc.df <- read.csv(data, row.names = 1)
View(wisc.df)
```

The diagnosis column will not be used in the in our analysis so we will omit the first column
```{r}
wisc.data <- wisc.df[,-1]
View(wisc.data)
```

We will save the data in the diagnosis column in a vector to be used later in our analysis
```{r}
diagnosis <- wisc.df[,1]
View(diagnosis)
```

> **Q1. How many observations are in this dataset?**

```{r}
#dim() gives number of rows and columns. 
dim(wisc.data)
#length() outputs length of vectors, lists, or factors. 
length(wisc.data)
#nrow() outputs number of rows
nrow(wisc.data)

length(diagnosis)
```
The wisc.data has 569 rows and 30 columns of data.  The diagnosis vector has 569 data values total.


> **Q2. How many of the observations have a malignant diagnosis?**

```{r}
#table() outputs a contingency table of displaying the amount of repeated inputs
table(diagnosis)
```
There are 212 observations that have a malignant diagnosis.


>**Q3. How many variables/features in the data are suffixed with _mean?**

```{r}
#grep() finds specific matches to the argument pattern in each element of character vectors
#This outputs which columns have the suffix"_mean"
grep("_mean", colnames(wisc.data))
length(grep("_mean", colnames(wisc.data)))
```
Using length() we can see that there are 10 observation suffized with "_mean".


#Let's do a PCA analysis on this dataset!

We need to check the mean and standard deviation of the features (columns) of the wisc.data to determine if the data should be scaled.
```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

We need to use scale=TRUE in this case for the PCA analysis as the columns data are on different scales.
```{r}
# Perform PCA on wisc.data
wisc.pr <- prcomp(wisc.data, scale=TRUE)
```

Now we will take a look at the summary of the results
```{r}
summary(wisc.pr)
```

> **Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?**

Based on the summary results, there is a 44.27% cumulative proportion captured by PC1.

> **Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?**

Based on the summary results, about 3 PCs are required to describe at least 70% of the original variance in the data.

> **Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?**

Based on the summary results, about 7 PCs are required to describe at least 90% of the original variance in the data.

#Interpretting PCA Results

We will create a some visualizations to help understand the PCA results.
We will create a biplot.

```{r}
biplot(wisc.pr)
```
> **Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?**

There is a big black blob of data inputs with the columns names spread out and pointing in the center.  It is not easy to understand because I do not know what the axis are scaled too to represent.  


# We will generate our own scatterplot to make sense of the PCA results

```{r}
# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=as.factor(diagnosis), xlab = "PC1",
     ylab ="PC2")
```

> **Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?**

```{r}
# Scatter plot observations by components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=as.factor(diagnosis), xlab = "PC1",
     ylab ="PC3")
```
PC2 captures more variance in the original data than PC3, so the first plot appears better for separating the two subgroups of benign (black) and malignant (red) samples.


# Create a more aesthetic figure using ggplot!

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

# Variance explained
We will produce scree plots showing the proportion of variance explained as the number of PCs increases.

First, we calculate the variance of each PC by squaring the sdev component of wisc.pr
```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
Then, we calculate the variance explained by each PC by dividing by the total variance explained of all PCs.
```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

# Communicating PCA Results
We will check our understanding of the PCA results like the loadings and variance explained.

> **Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?**

```{r}
wisc.pr$rotation["concave.points_mean", 1]
```
The component for the concave.points_mean is -0.2608538.

> **Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?**

```{r}
summary(wisc.pr)
```
Based on the summary, we would need minimum 4 PCs to capture at least 80% of the variance of the data.

# Hierarchical Clustering
The distance between all pairs of observations are computed.

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

Calculate the distance between all pairs in the new scaled dataset
```{r}
data.dist <- dist(data.scaled)
```

Create a hierarchical clustering model using complete linkage.
```{r}
wisc.hclust <- hclust(data.dist)
```


> **Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?**

```{r}
# Viewing the plot
plot(wisc.hclust)
#adding a line to view height at which 4 clusters are made
abline(h=19, col="red", lty=2)
```
Around height 19, the clustering model has 4 clusters

# Selecting number of clusters
We will compare the outputs from your hierarchical clustering model to the actual diagnoses.

```{r}
#using cutree to cut the tree to make 4 clusters
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)

#use table() function to compare the cluster membership to the actual diagnoses
table(wisc.hclust.clusters, diagnosis)
```

> **Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?**

```{r}
wisc.hclust.clusters2 <- cutree(wisc.hclust, k=2)
table(wisc.hclust.clusters2, diagnosis)

wisc.hclust.clusters3 <- cutree(wisc.hclust, k=3)
table(wisc.hclust.clusters3, diagnosis)

wisc.hclust.clusters5 <- cutree(wisc.hclust, k=5)
table(wisc.hclust.clusters5, diagnosis)

wisc.hclust.clusters6 <- cutree(wisc.hclust, k=6)
table(wisc.hclust.clusters6, diagnosis)

wisc.hclust.clusters7 <- cutree(wisc.hclust, k=7)
table(wisc.hclust.clusters7, diagnosis)

wisc.hclust.clusters8 <- cutree(wisc.hclust, k=8)
table(wisc.hclust.clusters8, diagnosis)

wisc.hclust.clusters9 <- cutree(wisc.hclust, k=9)
table(wisc.hclust.clusters9, diagnosis)

wisc.hclust.clusters10 <- cutree(wisc.hclust, k=10)
table(wisc.hclust.clusters10, diagnosis)
```
A lower number of clusters would provide better analyses.  2-5 clusters seem appropriate.

> **Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.**

```{r}
# single method
wisc.hclust.single <- hclust(data.dist, method= "single" )
plot(wisc.hclust.single)

# Complete method
wisc.hclust.complete <- hclust(data.dist, method= "complete" )
plot(wisc.hclust.complete)

# Average method
wisc.hclust.average <- hclust(data.dist, method= "average" )
plot(wisc.hclust.average)

# Ward.D2 method
wisc.hclust.ward.D2 <- hclust(data.dist, method= "ward.D2" )
plot(wisc.hclust.ward.D2)
```
They all appear crowded, others to a more confusing extent, but overall the ward.D2 method seems most similar in appearance and more clean.

# K-means clustering
We will create a k-means clustering model on the data and compare the results to the actual diagnoses and results of the hierarchical clustering model.

```{r}
#creating k-means with the scaled data created for the hierarchical clustering
#Making 2 clusters and running algorithm 20 times
wisc.km <- kmeans(data.scaled, centers=2, nstart= 20)

#use table() function to compare the cluster membership of the k-means model to the actual diagnoses contained in the diagnosis vector.
table(wisc.km$cluster, diagnosis)
```

> **Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?**

```{r}
table(wisc.km$cluster, wisc.hclust.clusters)
```
Clusters 1, 2, and 4 from the hierarchical clustering model can be interpreted as the cluster 1 for the k-means algorithm. Cluster 3 from the hierarchical clustering can be interpreted as the cluster 2 for k-means.

#Combining methods
We will apply PCA results to hierarchical clustering.

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")
plot(wisc.pr.hclust)
```
This appears better than our previous clustering results on the original scaled data. There are 2 main branches in this dendrogram indicating two clusters that could possibly represent the malignant and benign samples. 

```{r}
#creating 2 clusters and a table to view what samples are in each cluster
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)

#seeing if the 2 branches represent M and B samples
table(grps, diagnosis)

#plotting the results using grps to color 
plot(wisc.pr$x[,1:2], col=grps)


```

```{r}
#plotting the results using diagnosis vector to color 
plot(wisc.pr$x[,1:2], col=as.factor(diagnosis))
```

To match things, we can turn our groups into a factor and reorder the levels so cluster 2 comes first and gets the first color (black) and cluster 1 gets the second color (red).

```{r}
g <- as.factor(grps)
levels(g)

g <- relevel(g,2)
levels(g)

# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
#Use the distance along the first 7 PCs for clustering
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")

#cut 2 clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

> **Q15. How well does the newly created model with four clusters separate out the two diagnoses?**

```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```
Cluster 1 contains more malignant samples and cluster 2 has more benign.


> **Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.**

```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
#relooking at what the actual amount of M and B samples exist
table(diagnosis)
```
The k-means and hierarchical clustering both do significantly well in separating the M and B samples. K-means seems more similar to the separation of the actual diagnosis


# Sensitivity/Specificity

> **Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?**`

```{r}
table(wisc.hclust.clusters, diagnosis)
```

Best analysis for specificity would be the k-means model. And the best analysis for sensitivity would be the clustering model.


# Prediction
We will use the predict() function that will take our PCA model from the breat cancer dataset and new cancer cell data and project that data 

```{r}
#first we need to import the new data
new <- read.csv("new_samples.csv")

#predicting the data
npc <- predict(wisc.pr, newdata=new)
npc
```

Creating a new plot to compare the prediction
```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> **Q18. Which of these new patients should we prioritize for follow up based on your results?**

Patient 2
